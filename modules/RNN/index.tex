\chapter{Nonlinear filtering}
\label{ch:RNN}

The SWR detectors discussed up to this point -- both the band-pass and the GEVec-based filters -- calculate their output as a linear combination of input samples. This chapter explores whether a nonlinear method to calculate the output signal can improve SWR detection performance.


\section{Recurrent neural networks}

The nonlinear filtering algorithm that we choose to explore is the recurrent neural network (RNN). RNN's have advanced the state-of-the-art in many sequence and time-series processing tasks \cite{Greff2017}. (Examples include speech and language modelling \cite{LeCun2015,Goodfellow2016}; protein structure prediction \cite{Sonderby2014}; and diagnosis prediction from intensive care unit time series \cite{Lipton2015}).\footnotemark{} Relevant for SWR detection, RNN's can process multi-channel time-series, and calculating a new output sample has a relatively low cost\footnote{In contrast with very deep fully connected or convolutional neural networks.}, which is beneficial for real-time operation.

\footnotetext{For the reader with an interest in neuroscience, it might be of note that RNNs have recently been used in computational studies in various neuroscience subfields. Examples include \cite{Cueva2018} and \cite{Banino2018}, where an RNN was trained on a navigational task, after which the entries of the internal state vector of the RNN exhibited grid-cell-like activation patterns; \cite{Song2017}, who combined reinforcement learning theory with RNNs to model reward seeking and value-based computations; \cite{Li2017}, where RNNs were used to quantify and classify behaviour of the \emph{C. Elegans} model organism; and \cite{Guclu2017}, who modelled the fMRI hemodynamic response using an RNN. For an overview of artifical neural networks in general as models in neuroscience, see \cite{Kriegeskorte2015}.}


A recurrent neural network is a nonlinear dynamical system, driven by an input signal $\z_t \in \reals^C$. In the case of SWR detection, $\z_t$ is the multichannel LFP recording, with $C$ the number of channels. The RNN maintains an internal state vector $\h_t \in \reals^M$, which is updated at each discrete time step $t$ as:
%
\begin{equation}
\label{eq:RNN_f}
\h_t = f(\h\prev, \z_t).
\end{equation}
%
$f$ is a nonlinear function, parametrised by the coefficients of several affine transformations\footnote{Linear transformations (rotate, shear, scale), plus translations.} (see \cref{sec:GRU_eqs} for the complete update equations). The state vector $\h_t$ allows the RNN to maintain an efficient and task-relevant memory of past input samples \cite{LeCun2015}.\footnotemark{} We can read out the state vector of the RNN to obtain an output time series $n_t \in \reals$, with
%
\begin{equation}
\label{eq:RNN_g}
n_t = g(\h_t),
\end{equation}
%
where $g$ is again a parametrised nonlinear function.

\footnotetext{The vector $\h$ is also called the `hidden state', and its $M$entries are sometimes called `hidden units', `neurons', or `memory cells'.}

By tuning the coefficients of the affine transformations in $f$ and $g$ in an offline training phase, we can change the behaviour of the RNN so that its output $n_t$ for a given input signal $\z_t$ approaches a desired output $y_t$. (See \cref{sec:RNN-optim} for a description of this training phase, which involves the so called 'backpropagation through time' method). In the case of SWR detection, we can design a training signal $y_t$ that is for examples $1$ during SWR events in $\z_t$, and $0$ otherwise. If we avoid overfitting to the training data, we can use the trained RNN to detect SWR's in unseen signals $\z_t$. This is, in short, the proposed method of SWR detection in this chapter.



\section{Update equations}
\label{sec:GRU_eqs}

The specific RNN that we use for SWR detection uses so called \emph{gated recurrent units} and is called \emph{GRU} \cite{Cho2014}. It is closely related to the popular ``long short-term memory'' RNN (LSTM) \cite{Hochreiter1997,Greff2017}. The update equations for the GRU are simpler than those of the LSTM however, while achieving comparable performance \cite{Chung2014}.

The state-update function $f$ and the readout function $g$ are defined as follows for a GRU RNN:
%
\begin{align}
f:\ \h_t     &= \update \had \h\prev 
                + (1-\update) \had \hcand, \label{eq:hnew}\\
g:\ n_t  &= \sigma( \vb{w}_{hn}\trans \h_t + b_n )      \label{eq:out}\\
\end{align}
with
\begin{align}
\hcand  &= \tanh( \reset \had \W_{hh} \h\prev 
                   + \W_{zh} \z_t + \bias_h ),  \label{eq:hcand}\\
\update &= \sigma( \W_{hu} \h\prev 
                   + \W_{zu} \z_t + \bias_u),   \label{eq:update}\\
\reset  &= \sigma( \W_{hr} \h\prev 
                   + \W_{zr} \z_t + \bias_r ).  \label{eq:reset}
\end{align}
$\tanh$ is the hyperbolic tangent, and $\sigma$ is the logistic sigmoid, defined as $\sigma(x) = 1 / (1 + \exp(-x))$. They are applied point-wise, i.e. separately to each element of their argument vector. Similarly, ``$\had$'' denotes point-wise multiplication.  $\sigma$ compresses the real number line to $(0, 1)$, while $\tanh(x) = 2\ \sigma(x) - 1$ maps it to $(-1, 1)$. Therefore, for this RNN, $\h_t \in (-1, 1)^M$ and the output $n_t \in (0, 1)$.

Each element of the hidden state $\h_t$ is thus a weighted average of its existing value in $\h\prev$, and a candidate new value in $\hcand$. The weighting, in $\update$, depends on both the current input $\z_t$, and the full previous hidden state $\h\prev$.

Similarly, each element of the candidate new hidden state $\hcand$ is a function of both the current input $\z_t$, and all elements of the previous hidden state $\h\prev$. A so called ``reset'' multiplier, in $\reset$, determines how much the existing memory values in $\h\prev$ influence the candidate new memory value, compared to the influence of the current input $\z_t$. (When an element of $\reset$ is $0$, the corresponding hidden unit in $\hcand$ is only a function of the current input, and is thus decoupled from its past. The hidden unit in $\h_t$ can therefore be `reset').


% GRU
% Training
% 


\section{Optimization}
\label{sec:RNN-optim}



% \section{Regularization}\label{sec:RNN-regularization}
% \section{Channels}
% \section{Network size}
